{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1250\n",
      "Training indices shape: (1000,)\n",
      "Number of test samples: 250\n",
      "Number of overlapping images between training and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Define paths\n",
    "image_path = '/home/maria/Documents/HarvardData/Images'\n",
    "session_ims = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_images.p','rb'))\n",
    "\n",
    "# Construct full image paths\n",
    "image_paths = np.array([f\"{image_path}/{im.split('/')[2]}\" for im in session_ims])\n",
    "\n",
    "# Total number of images\n",
    "n_total = len(session_ims)\n",
    "print(f\"Total number of images: {n_total}\")\n",
    "\n",
    "# Define the number of training samples\n",
    "n_train = 1000\n",
    "\n",
    "# Ensure that n_train does not exceed n_total\n",
    "if n_train > n_total:\n",
    "    raise ValueError(\"Number of training samples exceeds the total number of available images.\")\n",
    "\n",
    "# Randomly select unique training indices without replacement\n",
    "training_path_inds = np.random.choice(n_total, size=n_train, replace=False)\n",
    "training_paths = image_paths[training_path_inds]\n",
    "\n",
    "# Determine test indices as those not in training_path_inds\n",
    "test_inds = np.setdiff1d(np.arange(n_total), training_path_inds)\n",
    "test_paths = image_paths[test_inds]\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training indices shape: {training_path_inds.shape}\")  # Should be (1000,)\n",
    "print(f\"Number of test samples: {len(test_paths)}\")           # Should be n_total - 1000\n",
    "\n",
    "# Optional: Verify no overlap between training and test sets\n",
    "overlap = np.intersect1d(training_paths, test_paths)\n",
    "print(f\"Number of overlapping images between training and test sets: {len(overlap)}\")  # Should be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid images found: 1250\n",
      "Number of training samples: 1000\n",
      "Number of test samples: 250\n",
      "Number of overlapping images between training and test sets: 0\n",
      "Augmented images have been saved to /home/maria/Documents/HarvardData/Augmentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ================================\n",
    "# Step 1: Define Data Augmentation Pipeline\n",
    "# ================================\n",
    "\n",
    "# Define your data augmentation pipeline (excluding normalization)\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),            # 50% chance to flip horizontally\n",
    "    transforms.RandomRotation(degrees=15),             # Rotate by Â±15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2,              # Adjust brightness\n",
    "                           contrast=0.2,                # Adjust contrast\n",
    "                           saturation=0.2,              # Adjust saturation\n",
    "                           hue=0.1)                     # Adjust hue\n",
    "    # Add more augmentations here if desired\n",
    "])\n",
    "\n",
    "# ================================\n",
    "# Step 2: Create Output Directory\n",
    "# ================================\n",
    "\n",
    "# Define the output directory for augmented images\n",
    "output_dir = '/home/maria/Documents/HarvardData/Augmentations'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ================================\n",
    "# Step 3: Load Training Image Paths\n",
    "# ================================\n",
    "\n",
    "# Load session image identifiers\n",
    "session_ims = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_images.p','rb'))\n",
    "\n",
    "# Function to find the correct image path with existing extension\n",
    "def find_image_path(session_ims, base_image_dir, possible_extensions):\n",
    "    image_paths = []\n",
    "    for p in session_ims:\n",
    "        # Extract the filename after 'OOD_monkey_data/Images/'\n",
    "        if 'OOD_monkey_data/Images/' in p:\n",
    "            filename = p.split('OOD_monkey_data/Images/')[-1]\n",
    "        else:\n",
    "            filename = os.path.basename(p)  # Fallback to basename\n",
    "        \n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Try each possible extension until a file is found\n",
    "        found_path = None\n",
    "        for ext_candidate in possible_extensions:\n",
    "            candidate_path = os.path.join(base_image_dir, base_name + ext_candidate)\n",
    "            if os.path.exists(candidate_path):\n",
    "                found_path = candidate_path\n",
    "                break\n",
    "        \n",
    "        if found_path is None:\n",
    "            # If no matching file is found, warn and skip\n",
    "            print(f\"Warning: No matching file found for base name: {base_name}\")\n",
    "            # Optionally, append a placeholder or handle as needed\n",
    "            # image_paths.append('/path/to/placeholder.jpg') # Uncomment if using placeholders\n",
    "        else:\n",
    "            image_paths.append(found_path)\n",
    "    \n",
    "    return np.array(image_paths)\n",
    "\n",
    "# Define base image directory and possible extensions\n",
    "base_image_dir = '/home/maria/Documents/HarvardData/Images'\n",
    "possible_extensions = ['.jpg', '.JPG', '.png', '.PNG']\n",
    "\n",
    "# Get the array of valid image paths\n",
    "image_paths = find_image_path(session_ims, base_image_dir, possible_extensions)\n",
    "print(f\"Total valid images found: {len(image_paths)}\")\n",
    "\n",
    "# Define the number of training samples\n",
    "n_train = 1000\n",
    "\n",
    "# Ensure that n_train does not exceed the total number of images\n",
    "if n_train > len(image_paths):\n",
    "    raise ValueError(\"Number of training samples exceeds the total number of available images.\")\n",
    "\n",
    "# Generate unique training indices without replacement\n",
    "training_path_inds = np.random.choice(len(image_paths), size=n_train, replace=False)\n",
    "training_paths = image_paths[training_path_inds]\n",
    "\n",
    "# Determine test indices as the complement of training indices\n",
    "test_inds = np.setdiff1d(np.arange(len(image_paths)), training_path_inds)\n",
    "test_paths = image_paths[test_inds]\n",
    "\n",
    "print(f\"Number of training samples: {len(training_paths)}\")  # Should be 1000\n",
    "print(f\"Number of test samples: {len(test_paths)}\")          # Should be len(image_paths) - 1000\n",
    "\n",
    "# Optional: Verify no overlap\n",
    "overlap = np.intersect1d(training_paths, test_paths)\n",
    "print(f\"Number of overlapping images between training and test sets: {len(overlap)}\")  # Should be 0\n",
    "\n",
    "# ================================\n",
    "# Step 4: Apply Augmentations and Save Augmented Images\n",
    "# ================================\n",
    "\n",
    "# Initialize a counter for naming augmented images\n",
    "counter = 1\n",
    "\n",
    "# Iterate through each image in the training set\n",
    "for path in training_paths:\n",
    "    try:\n",
    "        # Open the image and ensure it's in RGB format\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        \n",
    "        # Apply the augmentation transformations\n",
    "        augmented_image = augmentation_transform(image)\n",
    "        \n",
    "        # Extract the original file extension\n",
    "        _, ext = os.path.splitext(path)\n",
    "        ext = ext.lower()  # Ensure the extension is in lowercase (e.g., '.jpg', '.png')\n",
    "        \n",
    "        # Define the new filename (e.g., '1.jpg', '2.png', etc.)\n",
    "        new_filename = f\"{counter}{ext}\"\n",
    "        \n",
    "        # Define the full path to save the augmented image\n",
    "        save_path = os.path.join(output_dir, new_filename)\n",
    "        \n",
    "        # Save the augmented image\n",
    "        augmented_image.save(save_path)\n",
    "        \n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {path}: {e}\")\n",
    "\n",
    "print(f\"Augmented images have been saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/MousePipeline/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total augmented images found: 1000\n",
      "Processed 100/1000 images\n",
      "Processed 200/1000 images\n",
      "Processed 300/1000 images\n",
      "Processed 400/1000 images\n",
      "Processed 500/1000 images\n",
      "Processed 600/1000 images\n",
      "Processed 700/1000 images\n",
      "Processed 800/1000 images\n",
      "Processed 900/1000 images\n",
      "Processed 1000/1000 images\n",
      "Embeddings shape: (1000, 768)\n",
      "Embeddings have been saved to Bo220226_augmentations.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import torch\n",
    "\n",
    "# ================================\n",
    "# Step 1: Define Paths and Initialize ViT\n",
    "# ================================\n",
    "\n",
    "# Define the path to the augmented images\n",
    "augmentations_dir = '/home/maria/Documents/HarvardData/Augmentations'\n",
    "\n",
    "# Define the model name\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to GPU if available for faster processing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# ================================\n",
    "# Step 2: List and Sort Augmented Image Files\n",
    "# ================================\n",
    "\n",
    "# Define possible image extensions\n",
    "possible_extensions = ['.jpg', '.JPG', '.png', '.PNG']\n",
    "\n",
    "# List all image files in the augmentations directory with the specified extensions\n",
    "image_files = [f for f in os.listdir(augmentations_dir) if os.path.splitext(f)[1] in possible_extensions]\n",
    "\n",
    "# Function to extract numerical value from filename for sorting\n",
    "def extract_number(filename):\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    try:\n",
    "        return int(name)\n",
    "    except ValueError:\n",
    "        return name  # If filename isn't a number, sort lexicographically\n",
    "\n",
    "# Sort image files numerically\n",
    "image_files_sorted = sorted(image_files, key=extract_number)\n",
    "\n",
    "print(f\"Total augmented images found: {len(image_files_sorted)}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Embed Images and Extract CLS Tokens\n",
    "# ================================\n",
    "\n",
    "# Initialize a list to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through each image file\n",
    "for idx, image_file in enumerate(image_files_sorted, start=1):\n",
    "    image_path = os.path.join(augmentations_dir, image_file)\n",
    "    try:\n",
    "        # Open the image and ensure it's in RGB format\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Process the image using the ViTImageProcessor\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass through the ViT model to get outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract the CLS token from the pooler_output\n",
    "        cls_token = outputs.pooler_output.squeeze().cpu().numpy()  # Shape: [hidden_size]\n",
    "        \n",
    "        # Append the CLS token to the embeddings list\n",
    "        embeddings.append(cls_token)\n",
    "        \n",
    "        # Print progress every 100 images\n",
    "        if idx % 100 == 0 or idx == len(image_files_sorted):\n",
    "            print(f\"Processed {idx}/{len(image_files_sorted)} images\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "# ================================\n",
    "# Step 4: Compile and Save Embeddings\n",
    "# ================================\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array\n",
    "embeddings_array = np.vstack(embeddings)  # Shape: [num_images, hidden_size]\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_array.shape}\")  # Example: (5000, 768)\n",
    "\n",
    "# Define the path to save the embeddings\n",
    "save_path = 'Bo220226_augmentations.npy'\n",
    "\n",
    "# Save the embeddings array to a .npy file\n",
    "np.save(save_path, embeddings_array)\n",
    "\n",
    "print(f\"Embeddings have been saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered training images: 1000\n",
      "Number of filtered test images: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding of training images...\n",
      "Processed 32/32 batches\n",
      "Training embeddings shape: (1000, 768)\n",
      "Training embeddings have been saved to Bo220226_training_set.npy\n",
      "Starting embedding of test images...\n",
      "Processed 8/8 batches\n",
      "Test embeddings shape: (250, 768)\n",
      "Test embeddings have been saved to Bo220226_test_set.npy\n",
      "Loaded training embeddings shape: (1000, 768)\n",
      "Loaded test embeddings shape: (250, 768)\n",
      "Sample Training Embeddings:\n",
      "[[ 0.29927626 -0.6662772  -0.06322875 ...  0.5529889   0.20184693\n",
      "   0.7659803 ]\n",
      " [-0.41798595 -0.90310955  0.65945077 ...  0.32807255 -0.07639293\n",
      "   0.5208216 ]\n",
      " [-0.48322454 -0.3857926  -0.5082047  ...  0.64021987  0.7675375\n",
      "   0.8217118 ]\n",
      " [-0.56429875 -0.4239243  -0.4356303  ... -0.2282228   0.5815877\n",
      "   0.1060072 ]\n",
      " [ 0.5453199  -0.41448903  0.81488144 ... -0.42007238  0.84903145\n",
      "  -0.35274485]]\n",
      "Sample Test Embeddings:\n",
      "[[-0.6807091   0.14434983 -0.17385224 ... -0.06038163  0.5956547\n",
      "  -0.64094853]\n",
      " [-0.0127363   0.12923259 -0.29310498 ... -0.11654445  0.23687756\n",
      "   0.01215531]\n",
      " [-0.37209383 -0.79087687  0.02652562 ... -0.18684313  0.2878942\n",
      "   0.5711347 ]\n",
      " [-0.39395425 -0.16532838 -0.6054489  ... -0.2449874   0.04177381\n",
      "  -0.14031717]\n",
      " [-0.41369292 -0.65786743 -0.17238495 ...  0.4839693   0.09325565\n",
      "  -0.14438587]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ================================\n",
    "# Step 1: Define Allowed Extensions\n",
    "# ================================\n",
    "\n",
    "# Define possible image extensions\n",
    "possible_extensions = ['.jpg', '.JPG', '.png', '.PNG']\n",
    "\n",
    "# ================================\n",
    "# Step 2: Define Function to Find Correct Image Path\n",
    "# ================================\n",
    "\n",
    "def find_correct_image_path(base_dir, base_name, allowed_extensions):\n",
    "    \"\"\"\n",
    "    Given a base directory and base image name, find the correct file path by trying allowed extensions.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Directory where images are stored.\n",
    "        base_name (str): Base name of the image (without extension).\n",
    "        allowed_extensions (list): List of allowed file extensions.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Full path to the image if found, else None.\n",
    "    \"\"\"\n",
    "    for ext in allowed_extensions:\n",
    "        candidate_path = os.path.join(base_dir, base_name + ext)\n",
    "        if os.path.exists(candidate_path):\n",
    "            return candidate_path\n",
    "    print(f\"Warning: No matching file found for base name: {base_name}\")\n",
    "    return None\n",
    "\n",
    "# ================================\n",
    "# Step 3: Extract Base Names from session_ims\n",
    "# ================================\n",
    "\n",
    "# Example: Load session image identifiers (replace with your actual data loading)\n",
    "session_ims = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_images.p','rb'))\n",
    "\n",
    "# Define base image directory\n",
    "base_image_dir = '/home/maria/Documents/HarvardData/Images'\n",
    "\n",
    "# Function to extract base names from session_ims\n",
    "def extract_base_names(session_ims, prefix='OOD_monkey_data/Images/'):\n",
    "    base_names = []\n",
    "    for p in session_ims:\n",
    "        if prefix in p:\n",
    "            filename = p.split(prefix)[-1]\n",
    "            base_name, _ = os.path.splitext(filename)\n",
    "            base_names.append(base_name)\n",
    "        else:\n",
    "            # If prefix not found, use the basename without extension\n",
    "            base_name = os.path.splitext(os.path.basename(p))[0]\n",
    "            base_names.append(base_name)\n",
    "    return base_names\n",
    "\n",
    "# Extract base names\n",
    "base_names = extract_base_names(session_ims)\n",
    "\n",
    "# ================================\n",
    "# Step 4: Split into Training and Test Sets\n",
    "# ================================\n",
    "\n",
    "n_train = 1000\n",
    "np.random.seed(1337)\n",
    "train_indices = np.random.choice(len(base_names), size=n_train, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(base_names)), train_indices)\n",
    "\n",
    "training_base_names = [base_names[i] for i in train_indices]\n",
    "test_base_names = [base_names[i] for i in test_indices]\n",
    "\n",
    "# ================================\n",
    "# Step 5: Filter Image Paths with Correct Extensions\n",
    "# ================================\n",
    "\n",
    "def filter_image_paths(base_dir, base_names, allowed_extensions):\n",
    "    \"\"\"\n",
    "    Given a list of base names, find the correct image paths by trying allowed extensions.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Directory where images are stored.\n",
    "        base_names (list): List of base image names (without extension).\n",
    "        allowed_extensions (list): List of allowed file extensions.\n",
    "\n",
    "    Returns:\n",
    "        list: List of full image paths with correct extensions.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for base_name in base_names:\n",
    "        correct_path = find_correct_image_path(base_dir, base_name, allowed_extensions)\n",
    "        if correct_path:\n",
    "            image_paths.append(correct_path)\n",
    "        # If not found, it's already handled in find_correct_image_path\n",
    "    return image_paths\n",
    "\n",
    "# Get filtered training and test paths\n",
    "filtered_training_paths = filter_image_paths(base_image_dir, training_base_names, possible_extensions)\n",
    "filtered_test_paths = filter_image_paths(base_image_dir, test_base_names, possible_extensions)\n",
    "\n",
    "print(f\"Number of filtered training images: {len(filtered_training_paths)}\")\n",
    "print(f\"Number of filtered test images: {len(filtered_test_paths)}\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Initialize Processor and Model\n",
    "# ================================\n",
    "\n",
    "# Define the model name\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move the model to GPU if available for faster processing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# ================================\n",
    "# Step 7: Define the Embedding Function\n",
    "# ================================\n",
    "\n",
    "def embed_images(image_paths, processor, model, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Embed images using a Vision Transformer (ViT) model and extract CLS tokens.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths to embed.\n",
    "        processor (ViTImageProcessor): The image processor for ViT.\n",
    "        model (ViTModel): The pre-trained ViT model.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "        batch_size (int, optional): Number of images to process in a batch. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of CLS token embeddings with shape [num_images, hidden_size].\n",
    "    \"\"\"\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, image_paths, processor):\n",
    "            self.image_paths = image_paths\n",
    "            self.processor = processor\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_path = self.image_paths[idx]\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')  # Ensure 3 channels\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {image_path}: {e}\")\n",
    "                # Return a black image as a placeholder\n",
    "                image = Image.new('RGB', (224, 224))\n",
    "            return image\n",
    "\n",
    "    # Initialize the dataset and dataloader with a custom collate_fn\n",
    "    dataset = ImageDataset(image_paths, processor)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: x  # Custom collate_fn to return list of PIL Images\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate through the dataloader\n",
    "    for batch_idx, images in enumerate(dataloader, start=1):\n",
    "        # Process the batch using the processor\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "        # Move inputs to the appropriate device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract the CLS tokens\n",
    "        cls_tokens = outputs.pooler_output.squeeze().cpu().numpy()  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        embeddings.append(cls_tokens)\n",
    "\n",
    "        # Print progress every 100 batches or on the last batch\n",
    "        if batch_idx % 100 == 0 or batch_idx == len(dataloader):\n",
    "            print(f\"Processed {batch_idx}/{len(dataloader)} batches\")\n",
    "\n",
    "    # Concatenate all embeddings into a single array\n",
    "    embeddings_array = np.vstack(embeddings)  # Shape: [num_images, hidden_size]\n",
    "\n",
    "    return embeddings_array\n",
    "\n",
    "# ================================\n",
    "# Step 8: Embed and Save Training Images\n",
    "# ================================\n",
    "\n",
    "# Define the path where training embeddings will be saved\n",
    "training_embeddings_path = 'Bo220226_training_set.npy'\n",
    "\n",
    "# Embed the training images\n",
    "print(\"Starting embedding of training images...\")\n",
    "training_embeddings = embed_images(\n",
    "    image_paths=filtered_training_paths,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    batch_size=32  # Adjust based on your system's capabilities\n",
    ")\n",
    "print(f\"Training embeddings shape: {training_embeddings.shape}\")  # Expected: [n_train_filtered, hidden_size]\n",
    "\n",
    "# Save the embeddings to a .npy file\n",
    "np.save(training_embeddings_path, training_embeddings)\n",
    "print(f\"Training embeddings have been saved to {training_embeddings_path}\")\n",
    "\n",
    "# ================================\n",
    "# Step 9: Embed and Save Test Images\n",
    "# ================================\n",
    "\n",
    "# Define the path where test embeddings will be saved\n",
    "test_embeddings_path = 'Bo220226_test_set.npy'\n",
    "\n",
    "# Embed the test images\n",
    "print(\"Starting embedding of test images...\")\n",
    "test_embeddings = embed_images(\n",
    "    image_paths=filtered_test_paths,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    batch_size=32  # Adjust based on your system's capabilities\n",
    ")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")  # Expected: [n_test_filtered, hidden_size]\n",
    "\n",
    "# Save the embeddings to a .npy file\n",
    "np.save(test_embeddings_path, test_embeddings)\n",
    "print(f\"Test embeddings have been saved to {test_embeddings_path}\")\n",
    "\n",
    "# ================================\n",
    "# Step 10: Verification of Saved Embeddings\n",
    "# ================================\n",
    "\n",
    "# Load the training embeddings\n",
    "loaded_training_embeddings = np.load('Bo220226_training_set.npy')\n",
    "print(f\"Loaded training embeddings shape: {loaded_training_embeddings.shape}\")  # Should match [n_train_filtered, hidden_size]\n",
    "\n",
    "# Load the test embeddings\n",
    "loaded_test_embeddings = np.load('Bo220226_test_set.npy')\n",
    "print(f\"Loaded test embeddings shape: {loaded_test_embeddings.shape}\")  # Should match [n_test_filtered, hidden_size]\n",
    "\n",
    "# Optional: Inspect a few embeddings\n",
    "print(\"Sample Training Embeddings:\")\n",
    "print(loaded_training_embeddings[:5])\n",
    "\n",
    "print(\"Sample Test Embeddings:\")\n",
    "print(loaded_test_embeddings[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
