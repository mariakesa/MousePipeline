{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained (R²) on the training set: [0.9527528  0.9381931  0.96555436 0.9501871  0.943406   0.96696633\n",
      " 0.95197284 0.95345265 0.94243646 0.96255726 0.926697   0.9516869\n",
      " 0.9525622  0.95349556 0.9475264  0.9433099  0.9621101  0.94202834\n",
      " 0.956881   0.9372827  0.96248966 0.9478694  0.94123274 0.9663038\n",
      " 0.93710375 0.9557915  0.9467427  0.9403641  0.93300885 0.9386936\n",
      " 0.93176067 0.933885   0.9637079  0.94783753 0.94989383 0.9743374\n",
      " 0.9493982  0.9384288  0.9716013  0.9534048  0.9482876  0.956078\n",
      " 0.9471043  0.9520186  0.9496851  0.9530029  0.9418854  0.94002986\n",
      " 0.95057607 0.95658857 0.9579884  0.96112096 0.9460712  0.9479249\n",
      " 0.92547154 0.71191645 0.96214634 0.9478723  0.9291166  0.96552646\n",
      " 0.9309148  0.93525404 0.93012834 0.9540305 ]\n",
      "Variance explained (R²) on the test set: [-0.85414577 -0.78867185 -1.2811143  -1.3728731  -1.2536304  -0.35286176\n",
      " -0.4562074  -0.9741708  -1.4264975  -1.2995806  -0.6586654  -1.4967833\n",
      " -1.5886977  -1.3701291  -0.6749073  -0.5448824  -0.982445   -1.4169898\n",
      " -1.5551214  -1.0815265  -1.839247   -1.1343284  -0.69819796 -1.0610883\n",
      " -0.29002535 -1.3560879  -1.3756785  -1.215018   -0.2520237  -0.7555517\n",
      " -1.0482216  -1.3252769  -1.1693211  -0.6588459  -0.80196023 -0.57592857\n",
      " -0.9935641  -0.66287136 -1.1285706  -0.61529124 -1.220779   -0.4916376\n",
      " -0.92404497 -0.53520954 -2.3652818  -0.4887526  -1.2343788  -1.0628307\n",
      " -0.57683694 -0.8681532  -1.7945955  -0.903474   -0.38036942 -1.2365112\n",
      "  0.07146978 -0.4219315  -0.79794395 -0.7496916  -0.1684233  -1.1598244\n",
      " -0.7876911  -0.13831806 -0.7626965  -1.6761155 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Define the Mixture of Experts model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Define experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Define gating network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, num_experts),\n",
    "            nn.Softmax(dim=1)  # Outputs weights for each expert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get expert predictions\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: [batch_size, num_experts, output_size]\n",
    "        # Get gating weights\n",
    "        gating_weights = self.gate(x)  # Shape: [batch_size, num_experts]\n",
    "        # Combine expert outputs using gating weights\n",
    "        out = torch.sum(gating_weights.unsqueeze(2) * expert_outputs, dim=1)  # Weighted sum, Shape: [batch_size, output_size]\n",
    "        return out\n",
    "\n",
    "# Preprocess data\n",
    "X = torch.tensor(dino, dtype=torch.float32)  # Features\n",
    "y = torch.tensor(dat, dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "\n",
    "# Leave the last 20% of the data for testing\n",
    "n_samples = X.shape[0]\n",
    "n_train = int(0.8 * n_samples)\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "num_experts = 5  # Number of experts\n",
    "hidden_size = 64\n",
    "output_size = y.shape[1]  # Number of neurons\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MixtureOfExperts(input_size, num_experts, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    #if (epoch+1) % 10 == 0:\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / (n_train // batch_size):.4f}\")\n",
    "\n",
    "# Evaluate variance explained (R²)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train).numpy()  # Shape [n_train, n_neurons]\n",
    "    y_test_pred = model(X_test).numpy()    # Shape [n_test, n_neurons]\n",
    "\n",
    "variance_explained_train = r2_score(y_train.numpy(), y_train_pred, multioutput=\"raw_values\")\n",
    "variance_explained_test = r2_score(y_test.numpy(), y_test_pred, multioutput=\"raw_values\")\n",
    "\n",
    "print(f\"Variance explained (R²) on the training set: {variance_explained_train}\")\n",
    "print(f\"Variance explained (R²) on the test set: {variance_explained_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained (R²) on the training set for each neuron: [0.7970113655192915, 0.8056333321882915, 0.7126843482731282, 0.7678243344359418, 0.7405105986964463, 0.8111625779034071, 0.799938590419078, 0.7354060569333043, 0.7469261273240286, 0.7361924925784435, 0.792606718347316, 0.7604521667122904, 0.713761461112131, 0.7754371433003728, 0.7512113936169246, 0.8059645743612212, 0.7701486538240004, 0.6947529816472339, 0.7214695850181305, 0.7641990240870085, 0.7487137200767173, 0.7243423020773492, 0.807718972549369, 0.7382277282053423, 0.8286858715052748, 0.7259220434148288, 0.7555905076094305, 0.7587126161129287, 0.810729815645135, 0.7311482582607586, 0.7475223441913976, 0.7454013832376967, 0.7231105634930435, 0.7465019782402849, 0.7639901980218509, 0.7890528488990551, 0.7615541104625876, 0.8178673375526321, 0.7375746551612632, 0.8085261521977188, 0.7499719965427833, 0.8084209077294616, 0.782997048309156, 0.8149794066550237, 0.7258519441784792, 0.8161828636395753, 0.6675897866469795, 0.7629983913916565, 0.7851630974396357, 0.7905493564299703, 0.7142822731575101, 0.7126062910639057, 0.835172787243468, 0.7249896187969684, 0.8602514385657714, 0.6788425116059849, 0.782484862299989, 0.799598459848961, 0.8667682024369434, 0.6942176964292692, 0.7186641504954814, 0.8071366445820796, 0.7462454945766357, 0.7165240183999176]\n",
      "Variance explained (R²) on the test set for each neuron: [0.1654272401327891, 0.21573130211762404, 0.01875188228420155, 0.11130091766692474, 0.046727913579332925, 0.21100128794333994, 0.2624992515454455, 0.05697776135150401, 0.035736299264704896, 0.0027218373354902514, 0.2846373882744331, -0.047884875144935446, -0.10287396434839513, 0.042915618656974686, 0.10380563470912119, 0.1742917950992794, -0.021257610356492318, 0.013740055180294308, -0.10357486931987925, 0.021881033365210634, -0.08247111142238994, 0.0454625885944262, 0.07060356382958421, -0.005244060631522274, 0.2023096509731398, 0.026553278488423815, 0.15410327217487563, 0.11402196432155265, 0.2522323096207898, 0.094042559991669, 0.06955679185884978, -0.1292110724723703, 0.07563550562675792, 0.11923707001830286, 0.10211646094698856, 0.15537679748468425, 0.0822220154283938, 0.10134693905037839, -0.12408040320886426, 0.1754448805137777, 0.0028866750971303645, 0.17250366190351274, 0.03696527148683959, 0.2007401572990024, -0.020352510156628867, 0.2467319429106284, -0.06517018137321262, 0.056405031650586634, 0.17732205204963236, 0.13445562445852022, 0.007028394732588317, 0.07214171903219024, 0.24646564499448997, -0.044875614791273444, 0.2829171789987116, -0.1510504544932214, 0.16030966523238455, 0.17283608899923508, 0.25922003749398026, 0.03536656913496872, 0.13755406408689652, 0.2335503705756281, -0.06703300665985212, -0.03506167095689117]\n",
      "Mean R² on the training set: 0.7631\n",
      "Mean R² on the test set: 0.0819\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Preprocess data\n",
    "X = dino  # Features\n",
    "y = dat # Targets (neural activity) with shape [n_samples, n_neurons]\n",
    "\n",
    "# Leave the last 20% of the data for testing\n",
    "n_samples = X.shape[0]\n",
    "n_train = int(0.8 * n_samples)\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Initialize a GradientBoostingRegressor for each neuron\n",
    "n_neurons = y.shape[1]\n",
    "models = []\n",
    "r2_scores_train = []\n",
    "r2_scores_test = []\n",
    "\n",
    "#n_neurons=3\n",
    "# Train a separate model for each neuron\n",
    "for neuron_idx in range(n_neurons):\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    #print(neuron_idx)\n",
    "    model.fit(X_train, y_train[:, neuron_idx])  # Train on this neuron's activity\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict for this neuron\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² scores\n",
    "    r2_train = r2_score(y_train[:, neuron_idx], y_train_pred)\n",
    "    r2_test = r2_score(y_test[:, neuron_idx], y_test_pred)\n",
    "    \n",
    "    r2_scores_train.append(r2_train)\n",
    "    r2_scores_test.append(r2_test)\n",
    "\n",
    "# Print variance explained (R²) for each neuron\n",
    "print(\"Variance explained (R²) on the training set for each neuron:\", r2_scores_train)\n",
    "print(\"Variance explained (R²) on the test set for each neuron:\", r2_scores_test)\n",
    "\n",
    "# Mean R² across all neurons\n",
    "print(f\"Mean R² on the training set: {np.mean(r2_scores_train):.4f}\")\n",
    "print(f\"Mean R² on the test set: {np.mean(r2_scores_test):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
