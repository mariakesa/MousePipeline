{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained (R²) on the training set: [0.9527528  0.9381931  0.96555436 0.9501871  0.943406   0.96696633\n",
      " 0.95197284 0.95345265 0.94243646 0.96255726 0.926697   0.9516869\n",
      " 0.9525622  0.95349556 0.9475264  0.9433099  0.9621101  0.94202834\n",
      " 0.956881   0.9372827  0.96248966 0.9478694  0.94123274 0.9663038\n",
      " 0.93710375 0.9557915  0.9467427  0.9403641  0.93300885 0.9386936\n",
      " 0.93176067 0.933885   0.9637079  0.94783753 0.94989383 0.9743374\n",
      " 0.9493982  0.9384288  0.9716013  0.9534048  0.9482876  0.956078\n",
      " 0.9471043  0.9520186  0.9496851  0.9530029  0.9418854  0.94002986\n",
      " 0.95057607 0.95658857 0.9579884  0.96112096 0.9460712  0.9479249\n",
      " 0.92547154 0.71191645 0.96214634 0.9478723  0.9291166  0.96552646\n",
      " 0.9309148  0.93525404 0.93012834 0.9540305 ]\n",
      "Variance explained (R²) on the test set: [-0.85414577 -0.78867185 -1.2811143  -1.3728731  -1.2536304  -0.35286176\n",
      " -0.4562074  -0.9741708  -1.4264975  -1.2995806  -0.6586654  -1.4967833\n",
      " -1.5886977  -1.3701291  -0.6749073  -0.5448824  -0.982445   -1.4169898\n",
      " -1.5551214  -1.0815265  -1.839247   -1.1343284  -0.69819796 -1.0610883\n",
      " -0.29002535 -1.3560879  -1.3756785  -1.215018   -0.2520237  -0.7555517\n",
      " -1.0482216  -1.3252769  -1.1693211  -0.6588459  -0.80196023 -0.57592857\n",
      " -0.9935641  -0.66287136 -1.1285706  -0.61529124 -1.220779   -0.4916376\n",
      " -0.92404497 -0.53520954 -2.3652818  -0.4887526  -1.2343788  -1.0628307\n",
      " -0.57683694 -0.8681532  -1.7945955  -0.903474   -0.38036942 -1.2365112\n",
      "  0.07146978 -0.4219315  -0.79794395 -0.7496916  -0.1684233  -1.1598244\n",
      " -0.7876911  -0.13831806 -0.7626965  -1.6761155 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Define the Mixture of Experts model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Define experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Define gating network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, num_experts),\n",
    "            nn.Softmax(dim=1)  # Outputs weights for each expert\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get expert predictions\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: [batch_size, num_experts, output_size]\n",
    "        # Get gating weights\n",
    "        gating_weights = self.gate(x)  # Shape: [batch_size, num_experts]\n",
    "        # Combine expert outputs using gating weights\n",
    "        out = torch.sum(gating_weights.unsqueeze(2) * expert_outputs, dim=1)  # Weighted sum, Shape: [batch_size, output_size]\n",
    "        return out\n",
    "\n",
    "# Preprocess data\n",
    "X = torch.tensor(dino, dtype=torch.float32)  # Features\n",
    "y = torch.tensor(dat, dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "\n",
    "# Leave the last 20% of the data for testing\n",
    "n_samples = X.shape[0]\n",
    "n_train = int(0.8 * n_samples)\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "num_experts = 5  # Number of experts\n",
    "hidden_size = 64\n",
    "output_size = y.shape[1]  # Number of neurons\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MixtureOfExperts(input_size, num_experts, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    #if (epoch+1) % 10 == 0:\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / (n_train // batch_size):.4f}\")\n",
    "\n",
    "# Evaluate variance explained (R²)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train).numpy()  # Shape [n_train, n_neurons]\n",
    "    y_test_pred = model(X_test).numpy()    # Shape [n_test, n_neurons]\n",
    "\n",
    "variance_explained_train = r2_score(y_train.numpy(), y_train_pred, multioutput=\"raw_values\")\n",
    "variance_explained_test = r2_score(y_test.numpy(), y_test_pred, multioutput=\"raw_values\")\n",
    "\n",
    "print(f\"Variance explained (R²) on the training set: {variance_explained_train}\")\n",
    "print(f\"Variance explained (R²) on the test set: {variance_explained_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained (R²) on the training set for each neuron: [0.7970113655192915, 0.8056333321882914, 0.7126843482731282]\n",
      "Variance explained (R²) on the test set for each neuron: [0.16839211543954247, 0.21483946386258845, 0.007263491033722924]\n",
      "Mean R² on the training set: 0.7718\n",
      "Mean R² on the test set: 0.1302\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Preprocess data\n",
    "X = dino  # Features\n",
    "y = dat # Targets (neural activity) with shape [n_samples, n_neurons]\n",
    "\n",
    "# Leave the last 20% of the data for testing\n",
    "n_samples = X.shape[0]\n",
    "n_train = int(0.8 * n_samples)\n",
    "\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Initialize a GradientBoostingRegressor for each neuron\n",
    "n_neurons = y.shape[1]\n",
    "models = []\n",
    "r2_scores_train = []\n",
    "r2_scores_test = []\n",
    "\n",
    "#n_neurons=3\n",
    "# Train a separate model for each neuron\n",
    "for neuron_idx in range(n_neurons):\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    #print(neuron_idx)\n",
    "    model.fit(X_train, y_train[:, neuron_idx])  # Train on this neuron's activity\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict for this neuron\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R² scores\n",
    "    r2_train = r2_score(y_train[:, neuron_idx], y_train_pred)\n",
    "    r2_test = r2_score(y_test[:, neuron_idx], y_test_pred)\n",
    "    \n",
    "    r2_scores_train.append(r2_train)\n",
    "    r2_scores_test.append(r2_test)\n",
    "\n",
    "# Print variance explained (R²) for each neuron\n",
    "print(\"Variance explained (R²) on the training set for each neuron:\", r2_scores_train)\n",
    "print(\"Variance explained (R²) on the test set for each neuron:\", r2_scores_test)\n",
    "\n",
    "# Mean R² across all neurons\n",
    "print(f\"Mean R² on the training set: {np.mean(r2_scores_train):.4f}\")\n",
    "print(f\"Mean R² on the test set: {np.mean(r2_scores_test):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
