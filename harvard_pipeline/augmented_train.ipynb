{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1250\n",
      "Training indices shape: (1000,)\n",
      "Number of test samples: 250\n",
      "Number of overlapping images between training and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Define paths\n",
    "image_path = '/home/maria/Documents/HarvardData/Images'\n",
    "session_ims = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_images.p','rb'))\n",
    "\n",
    "# Construct full image paths\n",
    "image_paths = np.array([f\"{image_path}/{im.split('/')[2]}\" for im in session_ims])\n",
    "\n",
    "# Total number of images\n",
    "n_total = len(session_ims)\n",
    "print(f\"Total number of images: {n_total}\")\n",
    "\n",
    "# Define the number of training samples\n",
    "n_train = 1000\n",
    "\n",
    "# Ensure that n_train does not exceed n_total\n",
    "if n_train > n_total:\n",
    "    raise ValueError(\"Number of training samples exceeds the total number of available images.\")\n",
    "\n",
    "# Randomly select unique training indices without replacement\n",
    "training_path_inds = np.random.choice(n_total, size=n_train, replace=False)\n",
    "training_paths = image_paths[training_path_inds]\n",
    "\n",
    "# Determine test indices as those not in training_path_inds\n",
    "test_inds = np.setdiff1d(np.arange(n_total), training_path_inds)\n",
    "test_paths = image_paths[test_inds]\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training indices shape: {training_path_inds.shape}\")  # Should be (1000,)\n",
    "print(f\"Number of test samples: {len(test_paths)}\")           # Should be n_total - 1000\n",
    "\n",
    "# Optional: Verify no overlap between training and test sets\n",
    "overlap = np.intersect1d(training_paths, test_paths)\n",
    "print(f\"Number of overlapping images between training and test sets: {len(overlap)}\")  # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained (R²) on the training set: [0.8349749  0.8138107  0.42494756 0.73305345 0.68525225 0.842323\n",
      " 0.8772022  0.7027802  0.7135692  0.65651894 0.8669124  0.6618984\n",
      " 0.5850935  0.7388348  0.7991997  0.8575024  0.76986647 0.7032612\n",
      " 0.5775132  0.82820976 0.68129796 0.67123973 0.8602554  0.47174907\n",
      " 0.9310956  0.6202959  0.82132924 0.80876493 0.9222805  0.79376\n",
      " 0.83649087 0.7181926  0.6347059  0.7531076  0.80820084 0.7126083\n",
      " 0.72787476 0.84767586 0.5548817  0.82718146 0.6883861  0.8357412\n",
      " 0.7818674  0.8684794  0.6813617  0.90233374 0.65151197 0.76616913\n",
      " 0.8647225  0.8129101  0.68562996 0.68224835 0.9086205  0.6739396\n",
      " 0.957567   0.99481875 0.823471   0.8300817  0.9682656  0.5404942\n",
      " 0.79748523 0.92728287 0.8116495  0.633765  ]\n",
      "Variance explained (R²) on the test set: [-0.23375165 -0.1251105  -0.3243153  -0.8235593  -0.3815062  -0.10669672\n",
      " -0.31467128 -0.41491437 -0.48566186 -0.24415779 -0.1787399  -0.66482234\n",
      " -0.41370142 -0.6764833  -0.5380199  -0.21142936 -0.29658878 -0.77900445\n",
      " -0.49756002 -0.8934958  -0.8375056  -0.27735102 -0.39762557 -0.30277312\n",
      " -0.37751353 -0.4718083  -0.48707807 -0.58084404 -0.2656783  -0.60508204\n",
      " -0.9656292  -0.4415115  -0.43980455 -0.38502002 -0.58335614 -0.23659372\n",
      " -0.48847592 -0.06070089 -0.28602064 -0.35214925 -0.6819675  -0.39195633\n",
      " -0.40990388 -0.24737167 -0.6730094  -0.0608753  -0.5439894  -0.5583298\n",
      " -0.17803502 -0.24139857 -0.6457937  -0.5544511  -0.15303683 -0.84151435\n",
      " -0.11108148 -0.7505119  -0.20099664 -0.44694078 -0.17223513 -0.4717425\n",
      " -0.58825576 -0.24307597 -0.72054935 -0.5017053 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "vit_train=np.load(\"Bo220226_training_set.npy\")\n",
    "vit_test=np.load(\"Bo220226_test_set.npy\")\n",
    "# Define the Mixture of Experts model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Define the Mixture of Experts model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Define experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, output_size)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Define gating network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, num_experts),\n",
    "            nn.Softmax(dim=1)  # Outputs weights for each expert\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get expert predictions\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: [batch_size, num_experts, output_size]\n",
    "        # Get gating weights\n",
    "        gating_weights = self.gate(x)  # Shape: [batch_size, num_experts]\n",
    "        # Combine expert outputs using gating weights\n",
    "        out = torch.sum(gating_weights.unsqueeze(2) * expert_outputs, dim=1)  # Weighted sum, Shape: [batch_size, output_size]\n",
    "        return out\n",
    "\n",
    "# Preprocess data\n",
    "X_train = torch.tensor(vit_train, dtype=torch.float32)  # Features\n",
    "y_train = torch.tensor(dat[training_path_inds], dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "\n",
    "X_test = torch.tensor(vit_test, dtype=torch.float32)  # Features\n",
    "y_test = torch.tensor(dat[test_inds], dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "num_experts = 5  # Number of experts\n",
    "hidden_size = 64\n",
    "output_size = y_train.shape[1]  # Number of neurons\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MixtureOfExperts(input_size, num_experts, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    #if (epoch+1) % 10 == 0:\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / (n_train // batch_size):.4f}\")\n",
    "\n",
    "# Evaluate variance explained (R²)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train).numpy()  # Shape [n_train, n_neurons]\n",
    "    y_test_pred = model(X_test).numpy()    # Shape [n_test, n_neurons]\n",
    "\n",
    "variance_explained_train = r2_score(y_train.numpy(), y_train_pred, multioutput=\"raw_values\")\n",
    "variance_explained_test = r2_score(y_test.numpy(), y_test_pred, multioutput=\"raw_values\")\n",
    "\n",
    "print(f\"Variance explained (R²) on the training set: {variance_explained_train}\")\n",
    "print(f\"Variance explained (R²) on the test set: {variance_explained_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_set=np.load(\"/home/maria/MousePipeline/harvard_pipeline/Bo220226_augmentations.npy\")\n",
    "\n",
    "y_augmented = model(torch.tensor(augmented_set, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training features shape: torch.Size([2000, 768])\n",
      "Combined training targets shape: torch.Size([2000, 64])\n",
      "Training batches: 29\n",
      "Validation batches: 4\n",
      "MixtureOfExperts(\n",
      "  (experts): ModuleList(\n",
      "    (0-2): 3 x Sequential(\n",
      "      (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gate): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=3, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/MousePipeline/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 168\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    167\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 168\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    171\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/MousePipeline/.venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MousePipeline/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MousePipeline/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "vit_train=np.load(\"Bo220226_training_set.npy\")\n",
    "vit_test=np.load(\"Bo220226_test_set.npy\")\n",
    "# Define the Mixture of Experts model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "dat = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/session_responses.p', 'rb'))\n",
    "dino = pickle.load(open('/home/maria/Documents/HarvardData/processed_sessions_v3/Bo220226/dinov2_features.p', 'rb'))\n",
    "\n",
    "# Define the Mixture of Experts model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Define experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, output_size)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # Define gating network\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, num_experts),\n",
    "            nn.Softmax(dim=1)  # Outputs weights for each expert\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get expert predictions\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: [batch_size, num_experts, output_size]\n",
    "        # Get gating weights\n",
    "        gating_weights = self.gate(x)  # Shape: [batch_size, num_experts]\n",
    "        # Combine expert outputs using gating weights\n",
    "        out = torch.sum(gating_weights.unsqueeze(2) * expert_outputs, dim=1)  # Weighted sum, Shape: [batch_size, output_size]\n",
    "        return out\n",
    "\n",
    "# Preprocess data\n",
    "X_train = torch.tensor(vit_train, dtype=torch.float32)  # Features\n",
    "y_train = torch.tensor(dat[training_path_inds], dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "X_augmented=torch.tensor(augmented_set, dtype=torch.float32)\n",
    "# Concatenate the original training features with augmented features\n",
    "X_train_combined = torch.cat((X_train, X_augmented), dim=0)  # Shape: [n_train + n_augmented, feature_dim]\n",
    "\n",
    "# Concatenate the original training targets with augmented targets\n",
    "y_train_combined = torch.cat((y_train, y_augmented), dim=0)  #\n",
    "\n",
    "X_test = torch.tensor(vit_test, dtype=torch.float32)  # Features\n",
    "y_test = torch.tensor(dat[test_inds], dtype=torch.float32)  # Target with shape [n_samples, n_neurons]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "num_experts = 5  # Number of experts\n",
    "hidden_size = 64\n",
    "output_size = y_train.shape[1]  # Number of neurons\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MixtureOfExperts(input_size, num_experts, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    #if (epoch+1) % 10 == 0:\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / (n_train // batch_size):.4f}\")\n",
    "\n",
    "# Evaluate variance explained (R²)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train).numpy()  # Shape [n_train, n_neurons]\n",
    "    y_test_pred = model(X_test).numpy()    # Shape [n_test, n_neurons]\n",
    "\n",
    "variance_explained_train = r2_score(y_train.numpy(), y_train_pred, multioutput=\"raw_values\")\n",
    "variance_explained_test = r2_score(y_test.numpy(), y_test_pred, multioutput=\"raw_values\")\n",
    "\n",
    "print(f\"Variance explained (R²) on the training set: {variance_explained_train}\")\n",
    "print(f\"Variance explained (R²) on the test set: {variance_explained_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
